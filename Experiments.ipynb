{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import entropy\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "\n",
        "# Load and normalize stock market data\n",
        "def load_stock_market_data(file_path='Stock_Market.csv'):\n",
        "    data = pd.read_csv(file_path)\n",
        "    hutchison = data['Hutchison'].values\n",
        "    sun_hung_kai_prop = data['Sun_Hung_Kai_Prop'].values\n",
        "\n",
        "    # Normalize variables\n",
        "    X = (hutchison - np.mean(hutchison)) / np.std(hutchison)\n",
        "    Y = (sun_hung_kai_prop - np.mean(sun_hung_kai_prop)) / np.std(sun_hung_kai_prop)\n",
        "    return X, Y\n",
        "\n",
        "# CANM Model\n",
        "def CANM(X, Y):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "    # Model X -> Y\n",
        "    model_xy = GradientBoostingRegressor().fit(X_train.reshape(-1, 1), Y_train)\n",
        "    pred_y = model_xy.predict(X_test.reshape(-1, 1))\n",
        "    residual_xy = Y_test - pred_y\n",
        "    score_xy = np.var(residual_xy)\n",
        "\n",
        "    # Model Y -> X\n",
        "    model_yx = GradientBoostingRegressor().fit(Y_train.reshape(-1, 1), X_train)\n",
        "    pred_x = model_yx.predict(Y_test.reshape(-1, 1))\n",
        "    residual_yx = X_test - pred_x\n",
        "    score_yx = np.var(residual_yx)\n",
        "\n",
        "    return score_xy, score_yx\n",
        "\n",
        "# IGCI Model\n",
        "def IGCI(X, Y):\n",
        "    def calculate_entropy(data):\n",
        "        hist, _ = np.histogram(data, bins=50, density=True)\n",
        "        return entropy(hist)\n",
        "\n",
        "    hx = calculate_entropy(X)\n",
        "    hy = calculate_entropy(Y)\n",
        "    score_xy = hx - hy  # X -> Y\n",
        "    score_yx = hy - hx  # Y -> X\n",
        "\n",
        "    return score_xy, score_yx\n",
        "\n",
        "# ANM Using XGBoost\n",
        "def ANM_XGB(X, Y):\n",
        "    model_xy = XGBRegressor(n_estimators=100, learning_rate=0.1).fit(X.reshape(-1, 1), Y)\n",
        "    pred_y = model_xy.predict(X.reshape(-1, 1))\n",
        "    residual_xy = Y - pred_y\n",
        "    hsic_xy = np.var(residual_xy)\n",
        "\n",
        "    model_yx = XGBRegressor(n_estimators=100, learning_rate=0.1).fit(Y.reshape(-1, 1), X)\n",
        "    pred_x = model_yx.predict(Y.reshape(-1, 1))\n",
        "    residual_yx = X - pred_x\n",
        "    hsic_yx = np.var(residual_yx)\n",
        "\n",
        "    return hsic_xy, hsic_yx\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    X, Y = load_stock_market_data()\n",
        "\n",
        "    # Apply CANM\n",
        "    print(\"Running CANM...\")\n",
        "    score_xy, score_yx = CANM(X, Y)\n",
        "    print(f\"CANM Scores: X->Y: {score_xy:.4f}, Y->X: {score_yx:.4f}\")\n",
        "    print(\"CANM Inference:\", \"X -> Y\" if score_xy < score_yx else \"Y -> X\")\n",
        "\n",
        "    # Apply IGCI\n",
        "    print(\"\\nRunning IGCI...\")\n",
        "    score_xy, score_yx = IGCI(X, Y)\n",
        "    print(f\"IGCI Scores: X->Y: {score_xy:.4f}, Y->X: {score_yx:.4f}\")\n",
        "    print(\"IGCI Inference:\", \"X -> Y\" if score_xy < score_yx else \"Y -> X\")\n",
        "\n",
        "    # Apply ANM\n",
        "    print(\"\\nRunning ANM (XGBoost)...\")\n",
        "    hsic_xy, hsic_yx = ANM_XGB(X, Y)\n",
        "    print(f\"ANM Scores: X->Y: {hsic_xy:.4f}, Y->X: {hsic_yx:.4f}\")\n",
        "    print(\"ANM Inference:\", \"X -> Y\" if hsic_xy < hsic_yx else \"Y -> X\")\n",
        "\n",
        "\n",
        "# Load and normalize stock market data\n",
        "def load_stock_market_data(file_path='Stock_Market.csv'):\n",
        "    data = pd.read_csv(file_path)\n",
        "    hutchison = data['Hutchison'].values\n",
        "    sun_hung_kai_prop = data['Sun_Hung_Kai_Prop'].values\n",
        "\n",
        "    # Normalize variables\n",
        "    X = (hutchison - np.mean(hutchison)) / np.std(hutchison)\n",
        "    Y = (sun_hung_kai_prop - np.mean(sun_hung_kai_prop)) / np.std(sun_hung_kai_prop)\n",
        "    return X, Y\n",
        "\n",
        "# Post-Nonlinear Model (PNL)\n",
        "class PNLModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PNLModel, self).__init__()\n",
        "        self.nonlinear = nn.Sequential(\n",
        "            nn.Linear(1, 20), nn.ReLU(),\n",
        "            nn.Linear(20, 20), nn.ReLU(),\n",
        "            nn.Linear(20, 1)\n",
        "        )\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_nonlinear = self.nonlinear(x)\n",
        "        return self.linear(x) + x_nonlinear\n",
        "\n",
        "def train_pnl_model(X, Y, epochs=1000, lr=0.001):\n",
        "    X_tensor = torch.FloatTensor(X).view(-1, 1)\n",
        "    Y_tensor = torch.FloatTensor(Y).view(-1, 1)\n",
        "    model = PNLModel()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_tensor)\n",
        "        loss = criterion(output, Y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model, loss.item()\n",
        "\n",
        "# Loss-based Causal Inference\n",
        "class LossBasedModel(nn.Module):\n",
        "    def __init__(self, input_dim=1):\n",
        "        super(LossBasedModel, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 20), nn.ReLU(),\n",
        "            nn.Linear(20, 10), nn.ReLU(),\n",
        "            nn.Linear(10, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_loss_based_model(X, Y, epochs=1000, lr=0.001):\n",
        "    X_tensor = torch.FloatTensor(X).view(-1, 1)\n",
        "    Y_tensor = torch.FloatTensor(Y).view(-1, 1)\n",
        "    model = LossBasedModel()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_tensor)\n",
        "        loss = criterion(output, Y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model, loss.item()\n",
        "\n",
        "# Kernel-based Causal Inference\n",
        "def kernel_entropy_score(X, Y, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Compute entropy-based causal direction score using RBF kernels.\n",
        "    \"\"\"\n",
        "    def compute_rbf_kernel(X, gamma):\n",
        "        pairwise_sq_dists = np.square(X[:, None] - X[None, :])\n",
        "        return np.exp(-gamma * pairwise_sq_dists)\n",
        "\n",
        "    X = (X - np.mean(X)) / np.std(X)\n",
        "    Y = (Y - np.mean(Y)) / np.std(Y)\n",
        "\n",
        "    K_X = compute_rbf_kernel(X, gamma)\n",
        "    K_Y = compute_rbf_kernel(Y, gamma)\n",
        "\n",
        "    entropy_X = np.array([entropy(K_X[i]) for i in range(K_X.shape[0])])\n",
        "    entropy_Y = np.array([entropy(K_Y[i]) for i in range(K_Y.shape[0])])\n",
        "\n",
        "    return np.mean(entropy_X) - np.mean(entropy_Y)\n",
        "\n",
        "def kernel_causal_inference(X, Y, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Perform causal inference using kernel entropy scores.\n",
        "    \"\"\"\n",
        "    score_X_to_Y = kernel_entropy_score(X, Y, gamma)\n",
        "    score_Y_to_X = kernel_entropy_score(Y, X, gamma)\n",
        "\n",
        "    if score_X_to_Y > score_Y_to_X:\n",
        "        direction = \"X -> Y\"\n",
        "    else:\n",
        "        direction = \"Y -> X\"\n",
        "\n",
        "    return direction, score_X_to_Y, score_Y_to_X\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    X, Y = load_stock_market_data()\n",
        "\n",
        "    # Post-Nonlinear Model (PNL)\n",
        "    print(\"\\nRunning PNL Model...\")\n",
        "    _, loss_X_to_Y_pnl = train_pnl_model(X, Y)\n",
        "    _, loss_Y_to_X_pnl = train_pnl_model(Y, X)\n",
        "    pnl_direction = \"X -> Y\" if loss_X_to_Y_pnl < loss_Y_to_X_pnl else \"Y -> X\"\n",
        "    print(f\"PNL Loss (X -> Y): {loss_X_to_Y_pnl:.4f}\")\n",
        "    print(f\"PNL Loss (Y -> X): {loss_Y_to_X_pnl:.4f}\")\n",
        "    print(f\"PNL Inference: {pnl_direction}\")\n",
        "\n",
        "    # Loss-based Causal Inference\n",
        "    print(\"\\nRunning Loss-based Model...\")\n",
        "    _, loss_X_to_Y = train_loss_based_model(X, Y)\n",
        "    _, loss_Y_to_X = train_loss_based_model(Y, X)\n",
        "    loss_direction = \"X -> Y\" if loss_X_to_Y < loss_Y_to_X else \"Y -> X\"\n",
        "    print(f\"Loss-based Loss (X -> Y): {loss_X_to_Y:.4f}\")\n",
        "    print(f\"Loss-based Loss (Y -> X): {loss_Y_to_X:.4f}\")\n",
        "    print(f\"Loss-based Inference: {loss_direction}\")\n",
        "\n",
        "    # Kernel-based Causal Inference\n",
        "    print(\"\\nRunning Kernel-based Causal Inference...\")\n",
        "    kernel_direction, kernel_score_X_to_Y, kernel_score_Y_to_X = kernel_causal_inference(X, Y, gamma=0.5)\n",
        "    print(f\"Kernel Entropy Score (X -> Y): {kernel_score_X_to_Y:.4f}\")\n",
        "    print(f\"Kernel Entropy Score (Y -> X): {kernel_score_Y_to_X:.4f}\")\n",
        "    print(f\"Kernel-based Inference: {kernel_direction}\")\n"
      ],
      "metadata": {
        "id": "3qe04cRvVKfO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}