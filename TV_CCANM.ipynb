{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch import optim\n",
        "from scipy.stats import gaussian_kde\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CANM(nn.Module):\n",
        "    def __init__(self, N):\n",
        "        super(CANM, self).__init__()\n",
        "        self.N = N\n",
        "        # Fully connected layers for main encoding\n",
        "        self.fc1 = nn.Linear(3, 20)\n",
        "        self.fc21 = nn.Linear(20, 12)\n",
        "        self.fc22 = nn.Linear(12, 7)\n",
        "        self.fc23 = nn.Linear(7, N)\n",
        "\n",
        "        self.fc31 = nn.Linear(20, 12)\n",
        "        self.fc32 = nn.Linear(12, 7)\n",
        "        self.fc33 = nn.Linear(7, N)\n",
        "\n",
        "        # Fully connected layers for confounding variable encoding\n",
        "        self.fc_conf_mu1 = nn.Linear(20, 12)\n",
        "        self.fc_conf_mu2 = nn.Linear(12, 1)\n",
        "\n",
        "        self.fc_conf_logvar1 = nn.Linear(20, 12)\n",
        "        self.fc_conf_logvar2 = nn.Linear(12, 1)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc4 = nn.Linear(1 + N + 1, 10)\n",
        "        self.fc5 = nn.Linear(10, 7)\n",
        "        self.fc6 = nn.Linear(7, 5)\n",
        "        self.fc7 = nn.Linear(5, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def encode(self, data):\n",
        "        data = data.view(-1, 3)\n",
        "        h1 = self.relu(self.fc1(data))\n",
        "\n",
        "        # Main latent variable\n",
        "        h21 = self.relu(self.fc21(h1))\n",
        "        h22 = self.relu(self.fc22(h21))\n",
        "        mu = self.fc23(h22)\n",
        "\n",
        "        h31 = self.relu(self.fc31(h1))\n",
        "        h32 = self.relu(self.fc32(h31))\n",
        "        logvar = self.fc33(h32)\n",
        "\n",
        "        # Confounding variable latent space\n",
        "        conf_mu_hidden = self.relu(self.fc_conf_mu1(h1))\n",
        "        conf_mu = self.fc_conf_mu2(conf_mu_hidden)\n",
        "\n",
        "        conf_logvar_hidden = self.relu(self.fc_conf_logvar1(h1))\n",
        "        conf_logvar = self.fc_conf_logvar2(conf_logvar_hidden)\n",
        "\n",
        "        return mu, logvar, conf_mu, conf_logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps * std + mu\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, x, z, conf_z):\n",
        "        x = x.view(-1, 1)\n",
        "        z = z.view(-1, self.N)\n",
        "        conf_z = conf_z.view(-1, 1)\n",
        "        h4 = self.relu(self.fc4(torch.cat((x, z, conf_z), 1)))\n",
        "        h5 = self.relu(self.fc5(h4))\n",
        "        h6 = self.relu(self.fc6(h5))\n",
        "        yhat = self.fc7(h6)\n",
        "        return yhat\n",
        "\n",
        "    def forward(self, data):\n",
        "        data = data.view(-1, 3)\n",
        "        x, y, conf = data[:, 0], data[:, 1], data[:, 2]\n",
        "\n",
        "        # Encoding\n",
        "        mu, logvar, conf_mu, conf_logvar = self.encode(data)\n",
        "\n",
        "        # Reparameterization\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        conf_z = self.reparameterize(conf_mu, conf_logvar)\n",
        "\n",
        "        # Decoding\n",
        "        yhat = self.decode(x, z, conf_z)\n",
        "        return yhat, mu, logvar, conf_mu, conf_logvar\n",
        "\n",
        "class TransformerVAE(nn.Module):\n",
        "    def __init__(self, input_dim=3, latent_dim=10, confounding_dim=1, d_model=64, nhead=4, num_encoder_layers=4, num_decoder_layers=4):\n",
        "        super(TransformerVAE, self).__init__()\n",
        "\n",
        "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.fc_mu = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, latent_dim)\n",
        "        )\n",
        "\n",
        "        self.fc_logvar = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, latent_dim)\n",
        "        )\n",
        "\n",
        "        self.fc_conf_mu = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, confounding_dim)\n",
        "        )\n",
        "\n",
        "        self.fc_conf_logvar = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, confounding_dim)\n",
        "        )\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(1 + latent_dim + confounding_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.input_embedding(x)\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        encoded = self.transformer_encoder(x.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        mu = self.fc_mu(encoded)\n",
        "        logvar = self.fc_logvar(encoded)\n",
        "\n",
        "        conf_mu = self.fc_conf_mu(encoded)\n",
        "        conf_logvar = self.fc_conf_logvar(encoded)\n",
        "\n",
        "        return mu, logvar, conf_mu, conf_logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, x, z, conf_z):\n",
        "        yhat = self.decoder(torch.cat([x.view(-1, 1), z, conf_z], dim=1))\n",
        "        return yhat\n",
        "\n",
        "    def forward(self, data):\n",
        "        data = data.view(-1, 3)\n",
        "        x, y, u = data[:, 0], data[:, 1], data[:, 2]\n",
        "\n",
        "        mu, logvar, conf_mu, conf_logvar = self.encode(data)\n",
        "\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        conf_z = self.reparameterize(conf_mu, conf_logvar)\n",
        "\n",
        "        yhat = self.decode(x, z, conf_z)\n",
        "\n",
        "        return yhat, mu, logvar, conf_mu, conf_logvar\n",
        "\n",
        "def canm_loss_function(y, yhat, mu, logvar, sdy, beta, conf_mu, conf_logvar):\n",
        "    # Reconstruction loss (BCE)\n",
        "    N = y - yhat\n",
        "\n",
        "    if sdy.item() <= 0:\n",
        "        sdy = -sdy + 0.05\n",
        "\n",
        "    n = torch.distributions.Normal(0, sdy)\n",
        "    BCE = -torch.sum(n.log_prob(N))\n",
        "\n",
        "    # Regularization for the confounding variable (Confounder KL Divergence)\n",
        "    conf_reg = -0.5 * torch.sum(1 + conf_logvar - conf_mu.pow(2) - conf_logvar.exp()) * beta\n",
        "\n",
        "    # Regularization for the main latent space (KL Divergence)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * beta\n",
        "\n",
        "    # Total loss\n",
        "    return BCE + KLD + conf_reg\n",
        "\n",
        "\n",
        "def transformer_loss_function(y, yhat, mu, logvar, conf_mu, conf_logvar, sdy, beta):\n",
        "    N = y - yhat\n",
        "\n",
        "    if sdy.item() <= 0:\n",
        "        sdy = -sdy + 0.05\n",
        "\n",
        "    n = torch.distributions.Normal(0, sdy)\n",
        "    BCE = -torch.sum(n.log_prob(N))\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * beta\n",
        "    CONF_KLD = -0.5 * torch.sum(1 + conf_logvar - conf_mu.pow(2) - conf_logvar.exp()) * beta\n",
        "    return BCE + KLD + CONF_KLD\n"
      ],
      "metadata": {
        "id": "3qe04cRvVKfO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}